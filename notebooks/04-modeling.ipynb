{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-11T18:45:05.282381Z",
     "iopub.status.busy": "2024-12-11T18:45:05.281945Z",
     "iopub.status.idle": "2024-12-11T18:45:06.547388Z",
     "shell.execute_reply": "2024-12-11T18:45:06.546109Z",
     "shell.execute_reply.started": "2024-12-11T18:45:05.282329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from functools import partial\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from nba_betting_ai.model.bayesian import BayesianResultPredictor\n",
    "from nba_betting_ai.training.dataset import NBADataset\n",
    "from nba_betting_ai.training.pipeline import prepare_data\n",
    "from nba_betting_ai.data.processing import add_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_stats_season = [\n",
    "    'season_games',\n",
    "    'season_wins', 'season_pts_for', 'season_pts_against',\n",
    "    'season_wins_avg', 'season_pts_for_avg', 'season_pts_against_avg',\n",
    "]\n",
    "team_stats_last_5 = [\n",
    "    'last_5_wins',\n",
    "    'last_5_pts_for_avg', 'last_5_pts_against_avg',\n",
    "    'last_5_pts_for_total', 'last_5_pts_against_total',\n",
    "]\n",
    "target_general = ['away_win', 'score_final_diff']\n",
    "target_team = 'score_final'\n",
    "identification_cols = [\n",
    "    'game_id', 'home_team_abbreviation', 'away_team_abbreviation',\n",
    "]\n",
    "other_features = [\n",
    "    'home_score', 'away_score', 'time_remaining', 'score_diff'\n",
    "]\n",
    "\n",
    "prefix_home = partial(add_prefix, prefix='home', return_type='list')\n",
    "prefix_away = partial(add_prefix, prefix='away', return_type='list')\n",
    "all_features = identification_cols \\\n",
    "            + target_general \\\n",
    "            + prefix_home([target_team]) + prefix_away([target_team]) \\\n",
    "            + prefix_home(team_stats_season) + prefix_away(team_stats_season) \\\n",
    "            + prefix_home(team_stats_last_5) + prefix_away(team_stats_last_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params = {\n",
    "    'seasons': 1,\n",
    "    'seed': 66,\n",
    "    'test_size': 0.2,\n",
    "    'n': 20,\n",
    "    'frac': None\n",
    "}\n",
    "\n",
    "feature_cols = [\n",
    "    'away_season_wins_avg', 'home_season_wins_avg',\n",
    "    'away_season_pts_diff_avg', 'away_last_5_pts_diff_avg',\n",
    "    'home_season_pts_diff_avg', 'home_last_5_pts_diff_avg'\n",
    "]\n",
    "target_general = ['final_score_diff']\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.005,\n",
    "    'lr_decay': 0.99,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 10,\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    'embedding_dim': 8,\n",
    "    'team_hidden_dim': 32,\n",
    "    'team_layers': 2,\n",
    "    'res_hidden_dim': 32,\n",
    "    'res_layers': 2,\n",
    "    'time_scaling': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, iteration 0, loss: 35170408309237.64\n",
      "Epoch 0, iteration 100, loss: 111.22494950518345\n",
      "Epoch 0, iteration 200, loss: 10.719148237112844\n",
      "Epoch 0, iteration 300, loss: 10.840657154290172\n",
      "Epoch 0, iteration 400, loss: 10.959729530017647\n",
      "Epoch 0, iteration 500, loss: 10.77739941600765\n",
      "Epoch 0, iteration 600, loss: 10.942427944499148\n",
      "Epoch 0, iteration 700, loss: 10.939377909763587\n",
      "Epoch 0, iteration 800, loss: 10.939305602108739\n",
      "Epoch 0, iteration 900, loss: 10.945909113580115\n",
      "Epoch 0, iteration 1000, loss: 10.934320140084933\n",
      "Epoch 0, iteration 1100, loss: 10.759384246785395\n",
      "Epoch 0, iteration 1200, loss: 143335.97257447382\n",
      "Epoch 0, iteration 1300, loss: 106378.65348069805\n",
      "Epoch 0, iteration 1400, loss: 175484.2121340087\n",
      "Epoch 0, iteration 1500, loss: 135305.46831568453\n",
      "Epoch 0, iteration 1600, loss: 121071.23220117428\n",
      "Epoch 0, iteration 1700, loss: 161790.76566289747\n",
      "Epoch 0, iteration 1800, loss: 15.461259672591796\n",
      "Epoch 0, iteration 1900, loss: 11.32309884770233\n",
      "Epoch 0, iteration 2000, loss: 11.132492727810146\n",
      "Epoch 0, iteration 2100, loss: 11.118105706874422\n",
      "Epoch 0, iteration 2200, loss: 11.07308687911079\n",
      "Epoch 0, iteration 2300, loss: 11.736329286572001\n",
      "Epoch 0, iteration 2400, loss: 11.338793452014754\n",
      "Epoch 0, iteration 2500, loss: 11.183167749366536\n",
      "Epoch 0, iteration 2600, loss: 11.064455866881683\n",
      "Epoch 0, iteration 2700, loss: 11.060628936244012\n",
      "Epoch 0, iteration 2800, loss: 10.915130677378935\n",
      "Epoch 0, iteration 2900, loss: 11.967577526784124\n",
      "Epoch 0, iteration 3000, loss: 11.547747631910994\n",
      "Epoch 0, iteration 3100, loss: 10.996525919240334\n",
      "Epoch 0, iteration 3200, loss: 11.171347503191534\n",
      "Epoch 0, iteration 3300, loss: 11.09148981965913\n",
      "Epoch 0, iteration 3400, loss: 11.035449697348339\n",
      "Epoch 0, iteration 3500, loss: 11.633196327298421\n",
      "Epoch 0, iteration 3600, loss: 11.015388338954255\n",
      "Epoch 0, iteration 3700, loss: 10.788688110529705\n",
      "Epoch 0, iteration 3800, loss: 10.783820799906584\n",
      "Epoch 0, iteration 3900, loss: 51.832475570590375\n",
      "Epoch 0, iteration 4000, loss: 10.98212269560561\n",
      "Epoch 0, iteration 4100, loss: 10.663081546623685\n",
      "Epoch 0, iteration 4200, loss: 11.067215199012718\n",
      "Epoch 0, iteration 4300, loss: 10.92998013052169\n",
      "Epoch 0, iteration 4400, loss: 10.797656940315584\n",
      "Epoch 0, iteration 4500, loss: 10.787811067129205\n",
      "Epoch 0, iteration 4600, loss: 10.807674117433399\n",
      "Epoch 0, iteration 4700, loss: 10.963717676434086\n",
      "Epoch 0, iteration 4800, loss: 10.934823622237674\n",
      "Epoch 0, iteration 4900, loss: 10.96640134520451\n",
      "Epoch 0, iteration 5000, loss: 10.884424302469041\n",
      "Epoch 0, iteration 5100, loss: 11.14326460235564\n",
      "Epoch 0, iteration 5200, loss: 10.91204822235968\n",
      "Epoch 0, iteration 5300, loss: 10.745821194506826\n",
      "Epoch 0, iteration 5400, loss: 10.846300051251957\n",
      "Epoch 0, iteration 5500, loss: 10.793421550555523\n",
      "Epoch 0, iteration 5600, loss: 10.835836453763596\n",
      "Epoch 0, iteration 5700, loss: 11.008493850291623\n",
      "Epoch 0, iteration 5800, loss: 10.95418648230526\n",
      "Epoch 0, iteration 5900, loss: 10.7412150023147\n",
      "Epoch 0, iteration 6000, loss: 10.951850470150301\n",
      "Epoch 0, iteration 6100, loss: 10.6410852297404\n",
      "Epoch 1, iteration 0, loss: 10.942994259487191\n",
      "Epoch 1, iteration 100, loss: 11.00503705386825\n",
      "Epoch 1, iteration 200, loss: 10.764169569960181\n",
      "Epoch 1, iteration 300, loss: 10.861770047561084\n",
      "Epoch 1, iteration 400, loss: 46.80563582410376\n",
      "Epoch 1, iteration 500, loss: 11.50249447941609\n",
      "Epoch 1, iteration 600, loss: 11.365517081248946\n",
      "Epoch 1, iteration 700, loss: 11.721166821318283\n",
      "Epoch 1, iteration 800, loss: 10.999356427279633\n",
      "Epoch 1, iteration 900, loss: 11.090706961496092\n",
      "Epoch 1, iteration 1000, loss: 10.970207374788163\n",
      "Epoch 1, iteration 1100, loss: 10.974788459019884\n",
      "Epoch 1, iteration 1200, loss: 10.973880775285634\n",
      "Epoch 1, iteration 1300, loss: 10.650085892633491\n",
      "Epoch 1, iteration 1400, loss: 10.986122277912203\n",
      "Epoch 1, iteration 1500, loss: 11.763548755163939\n",
      "Epoch 1, iteration 1600, loss: 10.86248903969715\n",
      "Epoch 1, iteration 1700, loss: 10.963402234336417\n",
      "Epoch 1, iteration 1800, loss: 10.834738931596029\n",
      "Epoch 1, iteration 1900, loss: 10.954261474101198\n",
      "Epoch 1, iteration 2000, loss: 10.828132365979245\n",
      "Epoch 1, iteration 2100, loss: 10.94593783192929\n",
      "Epoch 1, iteration 2200, loss: 10.80146247919144\n",
      "Epoch 1, iteration 2300, loss: 10.959681570356242\n",
      "Epoch 1, iteration 2400, loss: 10.82233546391582\n",
      "Epoch 1, iteration 2500, loss: 10.929049877906678\n",
      "Epoch 1, iteration 2600, loss: 10.935130130880404\n",
      "Epoch 1, iteration 2700, loss: 10.871298679353426\n",
      "Epoch 1, iteration 2800, loss: 10.816428062636632\n",
      "Epoch 1, iteration 2900, loss: 10.859559640156277\n",
      "Epoch 1, iteration 3000, loss: 10.929448089945048\n",
      "Epoch 1, iteration 3100, loss: 10.563359609190318\n",
      "Epoch 1, iteration 3200, loss: 10.930153004401657\n",
      "Epoch 1, iteration 3300, loss: 11.596382604660679\n",
      "Epoch 1, iteration 3400, loss: 10.84165627103854\n",
      "Epoch 1, iteration 3500, loss: 10.948095988523262\n",
      "Epoch 1, iteration 3600, loss: 10.931093488752339\n",
      "Epoch 1, iteration 3700, loss: 11.090913377339827\n",
      "Epoch 1, iteration 3800, loss: 10.715955185205953\n",
      "Epoch 1, iteration 3900, loss: 11.044167437069333\n",
      "Epoch 1, iteration 4000, loss: 10.977986096165642\n",
      "Epoch 1, iteration 4100, loss: 10.853226445058501\n",
      "Epoch 1, iteration 4200, loss: 10.92641141098685\n",
      "Epoch 1, iteration 4300, loss: 10.974321628948177\n",
      "Epoch 1, iteration 4400, loss: 10.928122399172965\n",
      "Epoch 1, iteration 4500, loss: 10.788100451036849\n",
      "Epoch 1, iteration 4600, loss: 10.804437061647219\n",
      "Epoch 1, iteration 4700, loss: 10.936723521234745\n",
      "Epoch 1, iteration 4800, loss: 10.932408418355948\n",
      "Epoch 1, iteration 4900, loss: 10.923177356192536\n",
      "Epoch 1, iteration 5000, loss: 10.737303631456045\n",
      "Epoch 1, iteration 5100, loss: 10.893682220401871\n",
      "Epoch 1, iteration 5200, loss: 10.929446192895124\n",
      "Epoch 1, iteration 5300, loss: 10.663887412315095\n",
      "Epoch 1, iteration 5400, loss: 10.925343630850541\n",
      "Epoch 1, iteration 5500, loss: 10.926960628744194\n",
      "Epoch 1, iteration 5600, loss: 10.930128653455009\n",
      "Epoch 1, iteration 5700, loss: 10.671579254556569\n",
      "Epoch 1, iteration 5800, loss: 10.940720969596335\n",
      "Epoch 1, iteration 5900, loss: 10.66203659742031\n",
      "Epoch 1, iteration 6000, loss: 10.732778938407328\n",
      "Epoch 1, iteration 6100, loss: 11.01963977305372\n",
      "Epoch 2, iteration 0, loss: 10.752348625465174\n",
      "Epoch 2, iteration 100, loss: 10.923454264244864\n",
      "Epoch 2, iteration 200, loss: 10.517976925067654\n",
      "Epoch 2, iteration 300, loss: 10.542622369040732\n",
      "Epoch 2, iteration 400, loss: 11.040460639921907\n",
      "Epoch 2, iteration 500, loss: 10.929408981325079\n",
      "Epoch 2, iteration 600, loss: 10.735634097423407\n",
      "Epoch 2, iteration 700, loss: 10.788059789798034\n",
      "Epoch 2, iteration 800, loss: 10.78753787319198\n",
      "Epoch 2, iteration 900, loss: 10.696278612551202\n",
      "Epoch 2, iteration 1000, loss: 10.937278229558853\n",
      "Epoch 2, iteration 1100, loss: 10.595745549148816\n",
      "Epoch 2, iteration 1200, loss: 10.925274307074371\n",
      "Epoch 2, iteration 1300, loss: 10.656436121776427\n",
      "Epoch 2, iteration 1400, loss: 11.654340089022925\n",
      "Epoch 2, iteration 1500, loss: 11.33177982375751\n",
      "Epoch 2, iteration 1600, loss: 10.841224812770584\n",
      "Epoch 2, iteration 1700, loss: 10.624695629657136\n",
      "Epoch 2, iteration 1800, loss: 11.299128403844128\n",
      "Epoch 2, iteration 1900, loss: 10.947638161222965\n",
      "Epoch 2, iteration 2000, loss: 10.931801227163334\n",
      "Epoch 2, iteration 2100, loss: 10.724596275985819\n",
      "Epoch 2, iteration 2200, loss: 10.768145136350405\n",
      "Epoch 2, iteration 2300, loss: 13.296913941637243\n",
      "Epoch 2, iteration 2400, loss: 10.915688899036125\n",
      "Epoch 2, iteration 2500, loss: 10.938801351006072\n",
      "Epoch 2, iteration 2600, loss: 10.596594627553381\n",
      "Epoch 2, iteration 2700, loss: 10.73823066376369\n",
      "Epoch 2, iteration 2800, loss: 10.841005259740589\n",
      "Epoch 2, iteration 2900, loss: 11.062679789467593\n",
      "Epoch 2, iteration 3000, loss: 10.926265010464585\n",
      "Epoch 2, iteration 3100, loss: 10.928295538149758\n",
      "Epoch 2, iteration 3200, loss: 10.921232668270576\n",
      "Epoch 2, iteration 3300, loss: 10.864546386729929\n",
      "Epoch 2, iteration 3400, loss: 11.102762961603597\n",
      "Epoch 2, iteration 3500, loss: 10.87884553154142\n",
      "Epoch 2, iteration 3600, loss: 10.59640157026027\n",
      "Epoch 2, iteration 3700, loss: 10.702805783889165\n",
      "Epoch 2, iteration 3800, loss: 10.709416154322025\n",
      "Epoch 2, iteration 3900, loss: 10.687659962554449\n",
      "Epoch 2, iteration 4000, loss: 10.92690325704328\n",
      "Epoch 2, iteration 4100, loss: 10.812515513373365\n",
      "Epoch 2, iteration 4200, loss: 10.931968365167943\n",
      "Epoch 2, iteration 4300, loss: 10.925950205152493\n",
      "Epoch 2, iteration 4400, loss: 10.842185331269727\n",
      "Epoch 2, iteration 4500, loss: 10.84971654296444\n",
      "Epoch 2, iteration 4600, loss: 10.86057056904923\n",
      "Epoch 2, iteration 4700, loss: 10.862707901321945\n",
      "Epoch 2, iteration 4800, loss: 10.669591866523163\n",
      "Epoch 2, iteration 4900, loss: 10.930048644346488\n",
      "Epoch 2, iteration 5000, loss: 10.657217737938709\n",
      "Epoch 2, iteration 5100, loss: 10.792522119309403\n",
      "Epoch 2, iteration 5200, loss: 10.924869275542997\n",
      "Epoch 2, iteration 5300, loss: 10.72861738433216\n",
      "Epoch 2, iteration 5400, loss: 10.816747335044093\n",
      "Epoch 2, iteration 5500, loss: 10.926481952869617\n",
      "Epoch 2, iteration 5600, loss: 10.86323552787507\n",
      "Epoch 2, iteration 5700, loss: 10.924309055407196\n",
      "Epoch 2, iteration 5800, loss: 10.822358876748051\n",
      "Epoch 2, iteration 5900, loss: 10.930120287471308\n",
      "Epoch 2, iteration 6000, loss: 10.926285120947075\n",
      "Epoch 2, iteration 6100, loss: 10.715086609998632\n",
      "Epoch 3, iteration 0, loss: 10.656409480419105\n",
      "Epoch 3, iteration 100, loss: 10.923983935179793\n",
      "Epoch 3, iteration 200, loss: 10.858296100130987\n",
      "Epoch 3, iteration 300, loss: 10.921538311348684\n",
      "Epoch 3, iteration 400, loss: 10.685683383433325\n",
      "Epoch 3, iteration 500, loss: 10.815189133092094\n",
      "Epoch 3, iteration 600, loss: 10.204064395762149\n",
      "Epoch 3, iteration 700, loss: 11.07516386742017\n",
      "Epoch 3, iteration 800, loss: 10.52745083193529\n",
      "Epoch 3, iteration 900, loss: 11.133188296702787\n",
      "Epoch 3, iteration 1000, loss: 10.674049260014543\n",
      "Epoch 3, iteration 1100, loss: 10.741651106701681\n",
      "Epoch 3, iteration 1200, loss: 10.663797635968596\n",
      "Epoch 3, iteration 1300, loss: 10.935922899155997\n",
      "Epoch 3, iteration 1400, loss: 10.798433154891912\n",
      "Epoch 3, iteration 1500, loss: 10.771967248740035\n",
      "Epoch 3, iteration 1600, loss: 17.450162561858534\n",
      "Epoch 3, iteration 1700, loss: 10.998347464954398\n",
      "Epoch 3, iteration 1800, loss: 10.798018106335121\n",
      "Epoch 3, iteration 1900, loss: 10.568920022307333\n",
      "Epoch 3, iteration 2000, loss: 10.620504532835064\n",
      "Epoch 3, iteration 2100, loss: 10.856607289514391\n",
      "Epoch 3, iteration 2200, loss: 10.648653897694208\n",
      "Epoch 3, iteration 2300, loss: 10.481786536443447\n",
      "Epoch 3, iteration 2400, loss: 42.297543578297905\n",
      "Epoch 3, iteration 2500, loss: 103.53366349323544\n",
      "Epoch 3, iteration 2600, loss: 62.28885180211601\n",
      "Epoch 3, iteration 2700, loss: 54.837155436274045\n",
      "Epoch 3, iteration 2800, loss: 68.12573854540766\n",
      "Epoch 3, iteration 2900, loss: 72.0437764201267\n",
      "Epoch 3, iteration 3000, loss: 39.60060223794386\n",
      "Epoch 3, iteration 3100, loss: 63.01552999621249\n",
      "Epoch 3, iteration 3200, loss: 45.38830630085747\n",
      "Epoch 3, iteration 3300, loss: 44.97486200665439\n",
      "Epoch 3, iteration 3400, loss: 69.44814946709377\n",
      "Epoch 3, iteration 3500, loss: 33.98490753176266\n",
      "Epoch 3, iteration 3600, loss: 16.485079420130067\n",
      "Epoch 3, iteration 3700, loss: 39.72544966480668\n",
      "Epoch 3, iteration 3800, loss: 13.872507696687393\n",
      "Epoch 3, iteration 3900, loss: 10.925410161583603\n",
      "Epoch 3, iteration 4000, loss: 10.92644029738065\n",
      "Epoch 3, iteration 4100, loss: 10.926405890629558\n",
      "Epoch 3, iteration 4200, loss: 10.927781954187875\n",
      "Epoch 3, iteration 4300, loss: 10.935495941620138\n",
      "Epoch 3, iteration 4400, loss: 10.925151360373892\n",
      "Epoch 3, iteration 4500, loss: 10.804717081064757\n",
      "Epoch 3, iteration 4600, loss: 10.91093531205038\n",
      "Epoch 3, iteration 4700, loss: 10.931339197128949\n",
      "Epoch 3, iteration 4800, loss: 2406063.459716666\n",
      "Epoch 3, iteration 4900, loss: 75.60522191849992\n",
      "Epoch 3, iteration 5000, loss: 43.855441356652165\n",
      "Epoch 3, iteration 5100, loss: 64.84229839890388\n",
      "Epoch 3, iteration 5200, loss: 33.77871652048317\n",
      "Epoch 3, iteration 5300, loss: 99.34750565702288\n",
      "Epoch 3, iteration 5400, loss: 67.19621282097114\n",
      "Epoch 3, iteration 5500, loss: 37.5306657548516\n",
      "Epoch 3, iteration 5600, loss: 57.085264659399684\n",
      "Epoch 3, iteration 5700, loss: 38.20297330312046\n",
      "Epoch 3, iteration 5800, loss: 32.78880168940343\n",
      "Epoch 3, iteration 5900, loss: 42.1341664060722\n",
      "Epoch 3, iteration 6000, loss: 42.48826381244781\n",
      "Epoch 3, iteration 6100, loss: 43.7118006531972\n",
      "Epoch 4, iteration 0, loss: 32.14796936407878\n",
      "Epoch 4, iteration 100, loss: 48.041546253255575\n",
      "Epoch 4, iteration 200, loss: 28.21196518885985\n",
      "Epoch 4, iteration 300, loss: 40.0949329439904\n",
      "Epoch 4, iteration 400, loss: 20.188945142987237\n",
      "Epoch 4, iteration 500, loss: 37.562762473165314\n",
      "Epoch 4, iteration 600, loss: 24.67848736961904\n",
      "Epoch 4, iteration 700, loss: 33.18968940977763\n",
      "Epoch 4, iteration 800, loss: 44.8164976149287\n",
      "Epoch 4, iteration 900, loss: 19.07973803238791\n",
      "Epoch 4, iteration 1000, loss: 24.01676752566526\n",
      "Epoch 4, iteration 1100, loss: 13.96021002632639\n",
      "Epoch 4, iteration 1200, loss: 14.430495493017222\n",
      "Epoch 4, iteration 1300, loss: 16.69108889927221\n",
      "Epoch 4, iteration 1400, loss: 19.710043317694307\n",
      "Epoch 4, iteration 1500, loss: 12.170444131379968\n",
      "Epoch 4, iteration 1600, loss: 22.2143535514866\n",
      "Epoch 4, iteration 1700, loss: 22.639458407700932\n",
      "Epoch 4, iteration 1800, loss: 17.379365394982365\n",
      "Epoch 4, iteration 1900, loss: 12.412048312817584\n",
      "Epoch 4, iteration 2000, loss: 30.47961547310638\n",
      "Epoch 4, iteration 2100, loss: 15.11363832475857\n",
      "Epoch 4, iteration 2200, loss: 12.14823572360533\n",
      "Epoch 4, iteration 2300, loss: 16.504706665137615\n",
      "Epoch 4, iteration 2400, loss: 10.848949077860205\n",
      "Epoch 4, iteration 2500, loss: 12.576436037390254\n",
      "Epoch 4, iteration 2600, loss: 13.174101025533673\n",
      "Epoch 4, iteration 2700, loss: 18.961181813153864\n",
      "Epoch 4, iteration 2800, loss: 10.81047951533002\n",
      "Epoch 4, iteration 2900, loss: 11.35655001264433\n",
      "Epoch 4, iteration 3000, loss: 11.531489749911765\n",
      "Epoch 4, iteration 3100, loss: 12.667033466790931\n",
      "Epoch 4, iteration 3200, loss: 10.881304263767282\n",
      "Epoch 4, iteration 3300, loss: 8.424505002944212\n",
      "Epoch 4, iteration 3400, loss: 14.037382491254824\n",
      "Epoch 4, iteration 3500, loss: 10.040083431742557\n",
      "Epoch 4, iteration 3600, loss: 9.004028896701271\n",
      "Epoch 4, iteration 3700, loss: 10.589013602100987\n",
      "Epoch 4, iteration 3800, loss: 11.32980891778243\n",
      "Epoch 4, iteration 3900, loss: 5.899253382054237\n",
      "Epoch 4, iteration 4000, loss: 10.611934081960232\n",
      "Epoch 4, iteration 4100, loss: 10.59224738663615\n",
      "Epoch 4, iteration 4200, loss: 7.419620547502556\n",
      "Epoch 4, iteration 4300, loss: 9.141661254618697\n",
      "Epoch 4, iteration 4400, loss: 9.97822917069223\n",
      "Epoch 4, iteration 4500, loss: 9.872589256244169\n",
      "Epoch 4, iteration 4600, loss: 7.57385433079306\n",
      "Epoch 4, iteration 4700, loss: 9.319644394300058\n",
      "Epoch 4, iteration 4800, loss: 7.952429322148695\n",
      "Epoch 4, iteration 4900, loss: 8.81230678810207\n",
      "Epoch 4, iteration 5000, loss: 7.555399278016846\n",
      "Epoch 4, iteration 5100, loss: 7.295648162597379\n",
      "Epoch 4, iteration 5200, loss: 8.475569935947492\n",
      "Epoch 4, iteration 5300, loss: 7.890498333915782\n",
      "Epoch 4, iteration 5400, loss: 7.846064016435964\n",
      "Epoch 4, iteration 5500, loss: 9.626195124783308\n",
      "Epoch 4, iteration 5600, loss: 7.972483959734551\n",
      "Epoch 4, iteration 5700, loss: 7.2940580405409925\n",
      "Epoch 4, iteration 5800, loss: 6.745123187229166\n",
      "Epoch 4, iteration 5900, loss: 6.674973298505481\n",
      "Epoch 4, iteration 6000, loss: 7.423397892313314\n",
      "Epoch 4, iteration 6100, loss: 7.189456950932889\n",
      "Epoch 5, iteration 0, loss: 6.696827292285672\n",
      "Epoch 5, iteration 100, loss: 8.095226505926993\n",
      "Epoch 5, iteration 200, loss: 8.355776010887443\n",
      "Epoch 5, iteration 300, loss: 7.244885517850063\n",
      "Epoch 5, iteration 400, loss: 7.513114921354394\n",
      "Epoch 5, iteration 500, loss: 6.924443876890865\n",
      "Epoch 5, iteration 600, loss: 6.493346722195305\n",
      "Epoch 5, iteration 700, loss: 7.307979284310014\n",
      "Epoch 5, iteration 800, loss: 6.911977098256727\n",
      "Epoch 5, iteration 900, loss: 7.3750754659749225\n",
      "Epoch 5, iteration 1000, loss: 7.351066706591694\n",
      "Epoch 5, iteration 1100, loss: 6.749869484748796\n",
      "Epoch 5, iteration 1200, loss: 7.176774832927007\n",
      "Epoch 5, iteration 1300, loss: 7.225792355181337\n",
      "Epoch 5, iteration 1400, loss: 7.05825946555354\n",
      "Epoch 5, iteration 1500, loss: 6.586271680496892\n",
      "Epoch 5, iteration 1600, loss: 6.342084141990559\n",
      "Epoch 5, iteration 1700, loss: 6.855495907578106\n",
      "Epoch 5, iteration 1800, loss: 6.2860896974293965\n",
      "Epoch 5, iteration 1900, loss: 6.784010675970437\n",
      "Epoch 5, iteration 2000, loss: 7.109398633657021\n",
      "Epoch 5, iteration 2100, loss: 7.104588527177143\n",
      "Epoch 5, iteration 2200, loss: 7.877240722216396\n",
      "Epoch 5, iteration 2300, loss: 6.66435022348868\n",
      "Epoch 5, iteration 2400, loss: 6.879975600324255\n",
      "Epoch 5, iteration 2500, loss: 7.121564072025265\n",
      "Epoch 5, iteration 2600, loss: 6.660336446993708\n",
      "Epoch 5, iteration 2700, loss: 6.724847648575686\n",
      "Epoch 5, iteration 2800, loss: 6.625891003940388\n",
      "Epoch 5, iteration 2900, loss: 6.555025213981619\n",
      "Epoch 5, iteration 3000, loss: 6.622328647107843\n",
      "Epoch 5, iteration 3100, loss: 6.419373847505296\n",
      "Epoch 5, iteration 3200, loss: 6.6908651520941875\n",
      "Epoch 5, iteration 3300, loss: 6.906200399928396\n",
      "Epoch 5, iteration 3400, loss: 6.723250622972002\n",
      "Epoch 5, iteration 3500, loss: 6.795207199791076\n",
      "Epoch 5, iteration 3600, loss: 6.543813040874967\n",
      "Epoch 5, iteration 3700, loss: 6.473376834844219\n",
      "Epoch 5, iteration 3800, loss: 6.5689880937103435\n",
      "Epoch 5, iteration 3900, loss: 7.240981114270422\n",
      "Epoch 5, iteration 4000, loss: 6.409303468633404\n",
      "Epoch 5, iteration 4100, loss: 6.781960453372194\n",
      "Epoch 5, iteration 4200, loss: 7.3549916176746475\n",
      "Epoch 5, iteration 4300, loss: 6.935954115871253\n",
      "Epoch 5, iteration 4400, loss: 6.614251730850117\n",
      "Epoch 5, iteration 4500, loss: 6.704753505772713\n",
      "Epoch 5, iteration 4600, loss: 7.335850495807486\n",
      "Epoch 5, iteration 4700, loss: 7.033179903300454\n",
      "Epoch 5, iteration 4800, loss: 6.704076820046696\n",
      "Epoch 5, iteration 4900, loss: 6.554488936194\n",
      "Epoch 5, iteration 5000, loss: 6.737053458860781\n",
      "Epoch 5, iteration 5100, loss: 6.496636477090249\n",
      "Epoch 5, iteration 5200, loss: 7.324940876948304\n",
      "Epoch 5, iteration 5300, loss: 7.15441623072595\n",
      "Epoch 5, iteration 5400, loss: 6.764703456221367\n",
      "Epoch 5, iteration 5500, loss: 6.580897897577103\n",
      "Epoch 5, iteration 5600, loss: 6.677753185201851\n",
      "Epoch 5, iteration 5700, loss: 7.118541985565271\n",
      "Epoch 5, iteration 5800, loss: 6.593758734968087\n",
      "Epoch 5, iteration 5900, loss: 6.558775189745415\n",
      "Epoch 5, iteration 6000, loss: 6.473998784458286\n",
      "Epoch 5, iteration 6100, loss: 7.120609356584136\n",
      "Epoch 6, iteration 0, loss: 6.584683470908741\n",
      "Epoch 6, iteration 100, loss: 6.618351779400712\n",
      "Epoch 6, iteration 200, loss: 6.900741697810044\n",
      "Epoch 6, iteration 300, loss: 6.801934334806239\n",
      "Epoch 6, iteration 400, loss: 6.785205942549316\n",
      "Epoch 6, iteration 500, loss: 6.7603524418292\n",
      "Epoch 6, iteration 600, loss: 6.8601264028286515\n",
      "Epoch 6, iteration 700, loss: 7.298137642950579\n",
      "Epoch 6, iteration 800, loss: 7.555140880381937\n",
      "Epoch 6, iteration 900, loss: 6.676239777918116\n",
      "Epoch 6, iteration 1000, loss: 6.401402291991548\n",
      "Epoch 6, iteration 1100, loss: 6.790122826819351\n",
      "Epoch 6, iteration 1200, loss: 7.330736577803245\n",
      "Epoch 6, iteration 1300, loss: 7.061921031075455\n",
      "Epoch 6, iteration 1400, loss: 6.878465192932284\n",
      "Epoch 6, iteration 1500, loss: 6.530383725346061\n",
      "Epoch 6, iteration 1600, loss: 6.76394874860698\n",
      "Epoch 6, iteration 1700, loss: 6.407992897116066\n",
      "Epoch 6, iteration 1800, loss: 6.515279239115919\n",
      "Epoch 6, iteration 1900, loss: 7.398213925023841\n",
      "Epoch 6, iteration 2000, loss: 7.068851927710379\n",
      "Epoch 6, iteration 2100, loss: 6.6889957603663435\n",
      "Epoch 6, iteration 2200, loss: 6.600041672425915\n",
      "Epoch 6, iteration 2300, loss: 6.6509285154193485\n",
      "Epoch 6, iteration 2400, loss: 6.934661930588833\n",
      "Epoch 6, iteration 2500, loss: 7.258367072478338\n",
      "Epoch 6, iteration 2600, loss: 6.73927217721716\n",
      "Epoch 6, iteration 2700, loss: 7.028404174236009\n",
      "Epoch 6, iteration 2800, loss: 6.6789450989708\n",
      "Epoch 6, iteration 2900, loss: 7.158230177628926\n",
      "Epoch 6, iteration 3000, loss: 6.934186433269554\n",
      "Epoch 6, iteration 3100, loss: 7.140326017045673\n",
      "Epoch 6, iteration 3200, loss: 6.5138069743510485\n",
      "Epoch 6, iteration 3300, loss: 6.62594217773951\n",
      "Epoch 6, iteration 3400, loss: 6.87481260925129\n",
      "Epoch 6, iteration 3500, loss: 7.391349940178139\n",
      "Epoch 6, iteration 3600, loss: 6.781590736911538\n",
      "Epoch 6, iteration 3700, loss: 7.05918171058665\n",
      "Epoch 6, iteration 3800, loss: 6.971350721729763\n",
      "Epoch 6, iteration 3900, loss: 6.792505140014673\n",
      "Epoch 6, iteration 4000, loss: 6.93367797353097\n",
      "Epoch 6, iteration 4100, loss: 7.134809311782478\n",
      "Epoch 6, iteration 4200, loss: 6.850696445468534\n",
      "Epoch 6, iteration 4300, loss: 6.8300125414811435\n",
      "Epoch 6, iteration 4400, loss: 6.702210917207696\n",
      "Epoch 6, iteration 4500, loss: 7.054780033078206\n",
      "Epoch 6, iteration 4600, loss: 6.804473945559325\n",
      "Epoch 6, iteration 4700, loss: 7.132275689737193\n",
      "Epoch 6, iteration 4800, loss: 6.8363145109859005\n",
      "Epoch 6, iteration 4900, loss: 6.860518913547197\n",
      "Epoch 6, iteration 5000, loss: 6.576157891592482\n",
      "Epoch 6, iteration 5100, loss: 6.796272317520845\n",
      "Epoch 6, iteration 5200, loss: 7.011825058948271\n",
      "Epoch 6, iteration 5300, loss: 7.20441454225849\n",
      "Epoch 6, iteration 5400, loss: 6.822488294002566\n",
      "Epoch 6, iteration 5500, loss: 6.705921862209798\n",
      "Epoch 6, iteration 5600, loss: 6.933328035038679\n",
      "Epoch 6, iteration 5700, loss: 6.567987092074331\n",
      "Epoch 6, iteration 5800, loss: 6.861241067491619\n",
      "Epoch 6, iteration 5900, loss: 7.178702373275486\n",
      "Epoch 6, iteration 6000, loss: 6.756762479760043\n",
      "Epoch 6, iteration 6100, loss: 6.603039027799636\n",
      "Epoch 7, iteration 0, loss: 6.642886957173323\n",
      "Epoch 7, iteration 100, loss: 7.424779685521125\n",
      "Epoch 7, iteration 200, loss: 7.2935138172882725\n",
      "Epoch 7, iteration 300, loss: 6.849829194374173\n",
      "Epoch 7, iteration 400, loss: 6.742498736778011\n",
      "Epoch 7, iteration 500, loss: 6.856570803131211\n",
      "Epoch 7, iteration 600, loss: 6.803195457365652\n",
      "Epoch 7, iteration 700, loss: 7.423486791664168\n",
      "Epoch 7, iteration 800, loss: 6.65413423878569\n",
      "Epoch 7, iteration 900, loss: 7.060994239885769\n",
      "Epoch 7, iteration 1000, loss: 6.628605973608728\n",
      "Epoch 7, iteration 1100, loss: 6.867423566009003\n",
      "Epoch 7, iteration 1200, loss: 6.796284496824502\n",
      "Epoch 7, iteration 1300, loss: 7.26026321196039\n",
      "Epoch 7, iteration 1400, loss: 6.814169737543122\n",
      "Epoch 7, iteration 1500, loss: 6.633050802386975\n",
      "Epoch 7, iteration 1600, loss: 6.6861424952131046\n",
      "Epoch 7, iteration 1700, loss: 6.661706416530018\n",
      "Epoch 7, iteration 1800, loss: 7.231749089511933\n",
      "Epoch 7, iteration 1900, loss: 6.819347533372775\n",
      "Epoch 7, iteration 2000, loss: 6.672735550498927\n",
      "Epoch 7, iteration 2100, loss: 7.0037922654020255\n",
      "Epoch 7, iteration 2200, loss: 6.814262496686413\n",
      "Epoch 7, iteration 2300, loss: 6.615220944919148\n",
      "Epoch 7, iteration 2400, loss: 6.706483147157371\n",
      "Epoch 7, iteration 2500, loss: 6.845594341578135\n",
      "Epoch 7, iteration 2600, loss: 6.708561051788447\n",
      "Epoch 7, iteration 2700, loss: 6.6206186211734845\n",
      "Epoch 7, iteration 2800, loss: 6.621783737891425\n",
      "Epoch 7, iteration 2900, loss: 7.509168167858413\n",
      "Epoch 7, iteration 3000, loss: 6.845369221009409\n",
      "Epoch 7, iteration 3100, loss: 7.056923367110853\n",
      "Epoch 7, iteration 3200, loss: 6.943411751013613\n",
      "Epoch 7, iteration 3300, loss: 6.902436452840947\n",
      "Epoch 7, iteration 3400, loss: 7.9430849266069705\n",
      "Epoch 7, iteration 3500, loss: 6.748538497782442\n",
      "Epoch 7, iteration 3600, loss: 6.689651584750716\n",
      "Epoch 7, iteration 3700, loss: 6.9519454351833065\n",
      "Epoch 7, iteration 3800, loss: 6.825786949099481\n",
      "Epoch 7, iteration 3900, loss: 7.222578441362001\n",
      "Epoch 7, iteration 4000, loss: 6.973758656854638\n",
      "Epoch 7, iteration 4100, loss: 6.631852892449054\n",
      "Epoch 7, iteration 4200, loss: 7.007674106143083\n",
      "Epoch 7, iteration 4300, loss: 6.751145283836728\n",
      "Epoch 7, iteration 4400, loss: 6.62885823811355\n",
      "Epoch 7, iteration 4500, loss: 6.845624332008162\n",
      "Epoch 7, iteration 4600, loss: 6.907593139677553\n",
      "Epoch 7, iteration 4700, loss: 6.812824673191321\n",
      "Epoch 7, iteration 4800, loss: 6.771814005346554\n",
      "Epoch 7, iteration 4900, loss: 6.891557819126531\n",
      "Epoch 7, iteration 5000, loss: 6.656775404528531\n",
      "Epoch 7, iteration 5100, loss: 7.80546156455838\n",
      "Epoch 7, iteration 5200, loss: 7.025932107951516\n",
      "Epoch 7, iteration 5300, loss: 7.063784851860236\n",
      "Epoch 7, iteration 5400, loss: 7.166711146005561\n",
      "Epoch 7, iteration 5500, loss: 7.19932925046364\n",
      "Epoch 7, iteration 5600, loss: 6.8464033303108565\n",
      "Epoch 7, iteration 5700, loss: 7.017213407727694\n",
      "Epoch 7, iteration 5800, loss: 6.837082451572904\n",
      "Epoch 7, iteration 5900, loss: 6.6379374628126815\n",
      "Epoch 7, iteration 6000, loss: 7.288235412406402\n",
      "Epoch 7, iteration 6100, loss: 6.848962766969714\n",
      "Epoch 8, iteration 0, loss: 7.088447956747318\n",
      "Epoch 8, iteration 100, loss: 6.848424719007726\n",
      "Epoch 8, iteration 200, loss: 6.897560995298768\n",
      "Epoch 8, iteration 300, loss: 6.952471865192886\n",
      "Epoch 8, iteration 400, loss: 6.636311375465351\n",
      "Epoch 8, iteration 500, loss: 6.714384144168479\n",
      "Epoch 8, iteration 600, loss: 6.657219352003404\n",
      "Epoch 8, iteration 700, loss: 7.010849534507717\n",
      "Epoch 8, iteration 800, loss: 6.9021266982538805\n",
      "Epoch 8, iteration 900, loss: 7.075271626291013\n",
      "Epoch 8, iteration 1000, loss: 6.846641255475932\n",
      "Epoch 8, iteration 1100, loss: 7.2217312459288685\n",
      "Epoch 8, iteration 1200, loss: 6.767116340420978\n",
      "Epoch 8, iteration 1300, loss: 6.729234477672625\n",
      "Epoch 8, iteration 1400, loss: 6.908677966911216\n",
      "Epoch 8, iteration 1500, loss: 6.932878097032667\n",
      "Epoch 8, iteration 1600, loss: 7.0770710914933534\n",
      "Epoch 8, iteration 1700, loss: 7.026399041119902\n",
      "Epoch 8, iteration 1800, loss: 6.930208380792747\n",
      "Epoch 8, iteration 1900, loss: 6.607213446382014\n",
      "Epoch 8, iteration 2000, loss: 6.6127049730143295\n",
      "Epoch 8, iteration 2100, loss: 7.367329318238104\n",
      "Epoch 8, iteration 2200, loss: 7.0577636578243785\n",
      "Epoch 8, iteration 2300, loss: 6.794475731930033\n",
      "Epoch 8, iteration 2400, loss: 6.6771708980503846\n",
      "Epoch 8, iteration 2500, loss: 7.145152590458617\n",
      "Epoch 8, iteration 2600, loss: 6.7235746239900465\n",
      "Epoch 8, iteration 2700, loss: 6.702652265654175\n",
      "Epoch 8, iteration 2800, loss: 6.645197586837743\n",
      "Epoch 8, iteration 2900, loss: 6.744060468736948\n",
      "Epoch 8, iteration 3000, loss: 6.682599098586687\n",
      "Epoch 8, iteration 3100, loss: 6.722182193767674\n",
      "Epoch 8, iteration 3200, loss: 6.79972202357089\n",
      "Epoch 8, iteration 3300, loss: 7.118544843077075\n",
      "Epoch 8, iteration 3400, loss: 6.540696012993706\n",
      "Epoch 8, iteration 3500, loss: 6.96232649259629\n",
      "Epoch 8, iteration 3600, loss: 6.941149690534833\n",
      "Epoch 8, iteration 3700, loss: 7.001705922441019\n",
      "Epoch 8, iteration 3800, loss: 6.920551516896569\n",
      "Epoch 8, iteration 3900, loss: 6.634923250551141\n",
      "Epoch 8, iteration 4000, loss: 7.581101269926418\n",
      "Epoch 8, iteration 4100, loss: 6.8040869746322565\n",
      "Epoch 8, iteration 4200, loss: 6.79519459489555\n",
      "Epoch 8, iteration 4300, loss: 6.797836820501779\n",
      "Epoch 8, iteration 4400, loss: 6.934493468210009\n",
      "Epoch 8, iteration 4500, loss: 6.887803100318493\n",
      "Epoch 8, iteration 4600, loss: 6.659255315783818\n",
      "Epoch 8, iteration 4700, loss: 6.935269873453929\n",
      "Epoch 8, iteration 4800, loss: 6.760648424692464\n",
      "Epoch 8, iteration 4900, loss: 6.877155855371557\n",
      "Epoch 8, iteration 5000, loss: 7.184687441581712\n",
      "Epoch 8, iteration 5100, loss: 7.075998252921542\n",
      "Epoch 8, iteration 5200, loss: 6.871466155517104\n",
      "Epoch 8, iteration 5300, loss: 6.542080861808307\n",
      "Epoch 8, iteration 5400, loss: 6.994754042416869\n",
      "Epoch 8, iteration 5500, loss: 6.586595998761032\n",
      "Epoch 8, iteration 5600, loss: 6.7193045677320535\n",
      "Epoch 8, iteration 5700, loss: 6.803107755219105\n",
      "Epoch 8, iteration 5800, loss: 7.143257325943568\n",
      "Epoch 8, iteration 5900, loss: 7.283267389020644\n",
      "Epoch 8, iteration 6000, loss: 7.353149359312001\n",
      "Epoch 8, iteration 6100, loss: 6.540309031284053\n",
      "Epoch 9, iteration 0, loss: 6.496037862857292\n",
      "Epoch 9, iteration 100, loss: 6.876143296434151\n",
      "Epoch 9, iteration 200, loss: 6.936950439656451\n",
      "Epoch 9, iteration 300, loss: 6.892837505343312\n",
      "Epoch 9, iteration 400, loss: 6.759090144598304\n",
      "Epoch 9, iteration 500, loss: 7.230822272992902\n",
      "Epoch 9, iteration 600, loss: 6.918607488674782\n",
      "Epoch 9, iteration 700, loss: 7.625186574857806\n",
      "Epoch 9, iteration 800, loss: 6.6557750428633415\n",
      "Epoch 9, iteration 900, loss: 6.919117798161048\n",
      "Epoch 9, iteration 1000, loss: 7.234099125705825\n",
      "Epoch 9, iteration 1100, loss: 7.013398261273056\n",
      "Epoch 9, iteration 1200, loss: 6.811453644902061\n",
      "Epoch 9, iteration 1300, loss: 7.2899286287476555\n",
      "Epoch 9, iteration 1400, loss: 6.728314258299715\n",
      "Epoch 9, iteration 1500, loss: 6.783019776476859\n",
      "Epoch 9, iteration 1600, loss: 6.923809808861272\n",
      "Epoch 9, iteration 1700, loss: 6.8534176655516825\n",
      "Epoch 9, iteration 1800, loss: 6.4684235483286905\n",
      "Epoch 9, iteration 1900, loss: 6.529686596817333\n",
      "Epoch 9, iteration 2000, loss: 6.630016822916815\n",
      "Epoch 9, iteration 2100, loss: 6.8603582039432585\n",
      "Epoch 9, iteration 2200, loss: 6.876696497779442\n",
      "Epoch 9, iteration 2300, loss: 6.867463561145447\n",
      "Epoch 9, iteration 2400, loss: 6.659329216133655\n",
      "Epoch 9, iteration 2500, loss: 6.7175560277236555\n",
      "Epoch 9, iteration 2600, loss: 6.684525571224492\n",
      "Epoch 9, iteration 2700, loss: 6.435567754399944\n",
      "Epoch 9, iteration 2800, loss: 6.90249085085402\n",
      "Epoch 9, iteration 2900, loss: 6.410914013565679\n",
      "Epoch 9, iteration 3000, loss: 6.9389861369761086\n",
      "Epoch 9, iteration 3100, loss: 6.967115058452977\n",
      "Epoch 9, iteration 3200, loss: 7.194955457856704\n",
      "Epoch 9, iteration 3300, loss: 6.813560139636756\n",
      "Epoch 9, iteration 3400, loss: 6.760634908601636\n",
      "Epoch 9, iteration 3500, loss: 6.779476191714045\n",
      "Epoch 9, iteration 3600, loss: 6.4954641728819595\n",
      "Epoch 9, iteration 3700, loss: 6.868699452171771\n",
      "Epoch 9, iteration 3800, loss: 7.195176267826955\n",
      "Epoch 9, iteration 3900, loss: 6.478464600462802\n",
      "Epoch 9, iteration 4000, loss: 6.676935970407956\n",
      "Epoch 9, iteration 4100, loss: 6.838139674531462\n",
      "Epoch 9, iteration 4200, loss: 6.633870323571767\n",
      "Epoch 9, iteration 4300, loss: 6.733646674921108\n",
      "Epoch 9, iteration 4400, loss: 6.771943293251116\n",
      "Epoch 9, iteration 4500, loss: 7.41836162028647\n",
      "Epoch 9, iteration 4600, loss: 6.762296255541436\n",
      "Epoch 9, iteration 4700, loss: 6.5437465593069435\n",
      "Epoch 9, iteration 4800, loss: 6.459728861965344\n",
      "Epoch 9, iteration 4900, loss: 7.2805036816504245\n",
      "Epoch 9, iteration 5000, loss: 6.864838356949248\n",
      "Epoch 9, iteration 5100, loss: 6.549280859904366\n",
      "Epoch 9, iteration 5200, loss: 7.187574200123468\n",
      "Epoch 9, iteration 5300, loss: 6.708281186417405\n",
      "Epoch 9, iteration 5400, loss: 7.033407642534398\n",
      "Epoch 9, iteration 5500, loss: 6.844245130544398\n",
      "Epoch 9, iteration 5600, loss: 7.0249300651129225\n",
      "Epoch 9, iteration 5700, loss: 7.0567697524057875\n",
      "Epoch 9, iteration 5800, loss: 7.24308921205291\n",
      "Epoch 9, iteration 5900, loss: 6.900744939587877\n",
      "Epoch 9, iteration 6000, loss: 6.841657552841339\n",
      "Epoch 9, iteration 6100, loss: 6.979353705021157\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ResultPredictor.forward() got an unexpected keyword argument 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_loader):\n\u001b[1;32m     47\u001b[0m     data \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 48\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     50\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/nba-betting-ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/nba-betting-ai/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: ResultPredictor.forward() got an unexpected keyword argument 'y'"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'features': feature_cols,\n",
    "    'target': target_general,\n",
    "    'training': {\n",
    "        'params': params,\n",
    "        'model_config': model_config\n",
    "    }\n",
    "}\n",
    "\n",
    "data = prepare_data(**data_params)\n",
    "train_dataset = NBADataset(feature_cols, target_general, data.X_train, data.teams)\n",
    "test_dataset = NBADataset(feature_cols, target_general, data.X_test, data.teams)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BayesianResultPredictor(\n",
    "    team_count=len(data.teams),\n",
    "    team_features=len(feature_cols)//2,\n",
    "    **model_config\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "const = 0.5 * torch.log(2 * torch.tensor(torch.pi, dtype=torch.float64)).to(device)\n",
    "\n",
    "for epoch in range(params['epochs']):\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        data = {\n",
    "            k: v.to(device)\n",
    "            for k, v in data.items()\n",
    "        }\n",
    "        target = data.pop('y')\n",
    "        optimizer.zero_grad()\n",
    "        output = model(**data)\n",
    "        mu, logvar = torch.chunk(output, 2, dim=-1)\n",
    "        logvar = torch.clamp(logvar, min=-10, max=10)\n",
    "        loss = const + logvar + (target - mu)**2 / torch.exp(logvar)\n",
    "        loss = torch.mean(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch {epoch}, iteration {i}, loss: {loss.item()}')\n",
    "        learning_rate = learning_rate * params.get('lr_decay', 1.0)\n",
    "        \n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        data = {\n",
    "            k: v.to(device)\n",
    "            for k, v in data.items()\n",
    "        }\n",
    "        target = data.pop('y')\n",
    "        output = model(**data)\n",
    "        mu, a = torch.chunk(output, 2, dim=-1)\n",
    "        predicted = mu >= 0\n",
    "        \n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2151223,
     "sourceId": 4088094,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
